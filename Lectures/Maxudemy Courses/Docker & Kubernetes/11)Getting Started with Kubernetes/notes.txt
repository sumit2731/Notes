171)more Problems with Manual Depooyment

    refer slide 1.

    1)container migt crash and needs to be replaced.
    2)depending upon traffic or workload we might want to increase or decrease the container instances
    3)if we have multiple contianers then we need to distribute incoming traffic equally.

172)Why kubernetes
    slide no 2 & 3

173)What is kubernetes

    refer slide 4
    Kubernetes can help us with the problem outlined in the last lecture because with kubernetes,we have a way of defining our
    deployments,our scaling of containers and how containers should be monitored and replaced if they fail. We have a way of 
    defining all of that independent from the cloud service we're using.

    Because kubernetes is an open-source system and de-facto standard for managing container deployments and for orchestrating 
    containers.

    slide 6 - how kubernetes work.
    slide 7 - confog file
    this configuration would work with any cloud provider as long as it supports Kubernetes, or even if it doesn't support it you
    can manually install some Kubernetes software on any machine you own and then that software, if you want to call it like that,
    we'll be able to utilize this config file.

    And what's even better is the fact that you can actually merge certain cloud providers-specific options into this config file.
    So that if some cloud provider would require additional configuration, you could just add that configuration into that main 
    file And if you then ever want to use that file with another cloud provider,you just need to remove or replace the cloud 
    provider specific configurations with the  new cloud provider,and you don't have to rewrite the entire config file.

    That is the idea behind kubernetes, Having this standardized way of describing deployments.

    slide 8- what kubernetes is not.

    It's an open-source project.It's just a collection of concepts and a collection of software you could say, which together can 
    be used with any cloud provider.

    kubernetes is not just a software which you run on some machine, it's indeed that collection of concepts and tools which you'll
     see in actual throughout this course, which can help us with deployment on any provider of our choice.



     slide 5 - kubernetes is docker-compose for multi-machine setup

     you can think of kubernetes,as Docker-Compose for multiple machines because in its core, that is basically what it's about.
        Docker compose is a tool we learn about,which helps us manage multi container projects easily on our local machine.We could
        even use it on single container projects

    And kubernetes does this the same for multi machine setups because when you deploy your application,you do that by running the
    application across multiple computers, multiple machines,not just one machine.And kubernetes makes deploying your containers,
    monitoring and restarting them automatically across multiple machines very easy.So it's like docker-compose with some convenience
    deployment specific extra features for managing and running your dockerized, your containerized application on a multi machine
    setup

174)Kubernetes: Architecture & Core Concepts

    refer slide 9 - Kubernetes architecture 

    pods - it all starts with a container which we wanna deploy,which in the Kubernetes world is actually managed by a so-called pod.
        You can think of a pod as the smallest possible unit in the Kubernetes world, which you can define in some config file for
        kubernetes to create.pod simply holds the container.a pod is always able to hold multiple containers which might need to work
        together but the smallest possible unit is simply one pod which then in turn is responsible for running this container

    worker_node - This pod with the container inside of it,then itself, runs on a so-called worker node.So a worker node is the thing in
        the Kubernetes world which runs your containers in the end.And you can think of worker nodes as your machines,your virtual 
        instances.you can have more than one pod running on same worker node. for bigger applications we will have more than 1 worker 
        node which are then able to run your different pods.

        If you use Kubernetes to dynamically add and remove containers, and therefore pods, as traffic in and decreases these pods are 
        automatically distributed by Kubernetes across all available worker nodes.So you can have different and equal containers
        running on multiple worker nodes to distribute workload evenly.
    
    network - network layer between pod and worker_node, which decides how pod shiuld intercat with outside world.
    
    master node - master node simply another server another remote machine,which has this control plane running on it,which then is 
        responsible for interacting with the worker nodes and the pods running on them.
    
    cluster - Now all together, this forms a cluster.This forms a cluster of your master and worker nodes and therefore one network
        in which all these different parts are connected.And then your master node is able to send instructions to a cloud provider API,
        to tell that cloud provider to create it's cloud provider specific resources to replicate this desired big picture this end state
        on that cloud provider.

176)Closer Look at worker node
    See slide 10.

177)A Closer Look at the Master Node
    see slide 11.

