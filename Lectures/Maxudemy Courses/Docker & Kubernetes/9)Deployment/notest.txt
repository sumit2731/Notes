125)from dev to prod.
    see slide 2
    things to keep in mind -
        1) for bind mounts even though we may use them during development and we shouldn't use them in production.You will still learn
         in this section how does won't contradict the idea behind containers and how does won't lead to containers that are different 
         during development and production.
        
        2)your containerized apps might need different setups for development and for production which sounds strange at first
            after all the idea with containers was that we have one at the same environment.That's true but some applications
            like React apps for example simply need a build step where the code is converted and optimized and that builds step
            happens after development before you deploy the application.But even though we need this build step in this module
            you will all learn how you can still ensure that you ship containers which have reproducible environments and where 
            to code that work locally will also work once you deployed it

        3)So whilst you might be able to test everything let's say with Docker compose locally on one host machine for deployment you
         might consider splitting it across multiple servers or multiple host machines

        4)couple of lectures in this module where we will actually go for less control but also less responsibilities solution
            so that ultimately we as a developer have an easier time.


126)deployment processes and examples
    here we will deploy a simple nodejs application woth database to EC2 in aws.
    refer slide 4
        hosting provider - remote machine.


127)here we deploy a node js app without db into EC2.EC2 is a service offered by AWS which allows you to spin up your own remote hosting
    machines, you could say,Your own computers in the cloud.
    And we will go though three main steps to bring our Dockerized application to life on an EC2 instance.
        refer slide 6

    refer to code1 foler. first we build the image -
        docker build -t node-dep-example .

    then we run the container based on that image -

        docker run -d --rm --name node-dep -p 80:80 node-dep-example

128)bind mounts in prod
    refer slide 3. 
    while running container we did'nt specified any bind mounds infact we didi not use any volumes.

    In development so whilst we're working on the application,our container of course should encapsulate the runtime environment,but not 
    and that's important, not necessarily the code.

    In Prod ,There the idea really is that the container works standalone and that it does not depend on any surrounding setup on the 
    remote machine.The image and therefore the container based on the image should be the single source of truth you could say.That means
    we can rely on the fact that if we take that image and run a container based on it,we got everything this application needs.We don't 
    also need to move some source code into a special folder on that remote machine.That would totally destroy the idea behind containers
    if we now all of a sudden again need to configure the hosting machine.We want to have everything,what our application needs inside of
    that container.There should be nothing, absolutely nothing,around that container on the hosting machine.And therefore when we build for
    production,we use copy instead of bind mounts.


    now we specify bind mounts and volumnes while running a container based on a image. thsi helps us to use same docker imge in dev and
    prod, its just while running container for dev we can use bind mounts. this is the reason why bind mounts an dvolumes are not part of
    dockerFile.


    Now of course if we used a Docker compose file,we might have that bind mount written into that compose file,but I will come back to 
    compose files and how we deploy such multi container projects later.For the moment it's about one container, one image and then copy
    is your friend


129)Introducing AWS & EC2
    EC2 allows us to create virtual servers in cloud which means our own computers in cloud.  
    1)go to aws management console.
    2)search Ec2 and clicked on it.
    3)now goal is to start a Ec2 instance, there are multiplw ways of doing it.one is to click on "Launch Instance".

130)Connecting to an EC2 instance
    1)here we configure our instance that we are going to launch. 
    2)important screen is keypair.Here you can create a new key pair and you will need this key pair,which will be a file in the end,to 
        later connect to your instance via SSH and to run commands on it. so created a key pair and gave it name example-1.
    3)after creating key-pair, we clicked on Luanch instance.
    4)click on "view all instances".
    5)status of your instnace your be running.
    6)now we need to connect to this server using ssh.
        a)for mac and linux - 
            you can directly use ssh command in terminal.
        b)for windows this is not build it .
            a)you need to instal wsl2 on widnows 10(linux running for windows) or putty.

    7)select instnace and click on connect and select ssh. here follow the steps given in to connect to server.
    8)now we are connected to remote server.

131)Installing docker
    updating packages -
        sudo yum update -y
            this will update all packages. then run this -

    installing docker -
        sudo amazon-linux-extras install docker

        that's why we should pick this amaxon image when we created this instance.on this amazozn based virtual instances,we have these commands 
        avalible which makes installing these extra softwares like docker easier. this will install docker on remote machine.

    starting docker -
        sudo service docker start
        now docker is started and we can run docker commands.


133)pushing image to cloud

    step to publish image to docker hub -

        1)create repository on docker hub. name it - sumeet-node-example-27.
        2)create a image on your local machine whose ame is same as name of repository. -

            docker build -t node-dep-example-1 .
            docker tag node-dep-example-1 sumeet27/sumeet-node-example-27

        3)then login into docker hub -
            docker login
        4)docker push sumeet27/sumeet-node-example-27

134)RUNNING & PUBLISHING the app(on EC2)
    run this command to download and run your pushed image on ec2 instance -

        sudo docker run -d --rm -p 80:80 sumeet27/sumeet-node-example-27

    you can run this to cnfoirm if your container is running - sudo docker run -d --rm -p 80:80 sumeet27/sumeet-node-example-27

    now go to instance ;ist in ec2 list and copy the ip4 address fo your instance. hit tha address in browser. but we do not see anything.
    
    And that's not a bug, that's a security feature.By default, your instance your EC2 instance, is basically disconnected from everything
    in the world wide web, so that no one is able to connect except for you with SSH.And this is controlled with a so called security group.

    to chnage this, on left side go to "Security Groups" or in your instnace you can see security group, you can click on it and you will be
    taken to security group.security groups basically controls which traffic is allowed on our EC2 instance.
        outbound rules - controls which traffic is allowed from the instance queue to somewhere else
        inbound rules - controls which traffic is allowed form outside world to this instance. here only one port is open which is 22, this
            is for ssh. this port is open for entire world. now here we add a new role to inbound group. to allow outside world to access this
            port.

    now using IP you can see your websit via ip of EC2.


135)managing and updating the container /image
    here we saw how you could push updates to your code,to that remote server and how you can then stop and shut everything down if you want
     that to do that. 

    updating the code -

        1)make code chnages on local system.
        2)build the image on local.
            docker build -t node-dep-example-1 .
            docker tag node-dep-example-1 sumeet27/sumeet-node-example-27
        3)push the image on docker hub.
            docker push sumeet27/sumeet-node-example-27

        4)stop container on EC2 instance.

        5)pull latest image -
            docker pull sumeet27/sumeet-node-example-27
        5)rerun the container based on image -
            sudo docker run -d --rm -p 80:80 sumeet27/sumeet-node-example-27

    Stopping Everything -
        Way 1 -
            a)stop the container.

        way 2 -
            a)terminate ecs instance - select the instance, actions -> instance state -> terminate


        this will not be the final way of deploying an application with Docker.Because it turns out that this approach,which has showed 
        you in great detail here,has a couple of disadvantages and downsides.


136)Disadvantages of current approach - 
    see slide 9.

    disdavantges -
        1)You are responsible for security.
        2)You are responsible for updating OS.
        3)connecting to remote client via SSH can be cumbersome. we can have some approach where  wwe can run a single command where all this
            will be taken care of.

137)From manualdeployment to managed services
    ECS - elastic container service. it helps in managing container.With launching them, running them, monitoring them and so on.

    switching to some managed service,no matter if it's by AWS or by any other provider,means that you don't just use Docker anymore,
    instead, you now use a service provided by a cloud provider and you have to follow the rules of that service you could say.
    That simply means that deploying containers and running containers is now not done with the Docker command anymore because now we don't
    install Docker on some machine anymore that's the entire idea behind a managed service,but instead it means that we now will need to 
    use the tools the cloud provider gives us for the specific service we wanna use.

    here we will show this with the example of AWS ECS.It's just one possible example.
    I still wanna show you over the next lectures,how you could take what you learned and migrate that into the rule set
    a specific service gives you or forces on you and therefore these steps shown there will only work for that service.

139)Deploying with AWS ECS: A managed Docker container Service.

    here we want to run our container based on image pushed in docker hub via ECS. AWS ECS things in 4 categories -

    a)clusters
    b)containers
    c)tasks
    d)service

    containaer -
        lets start with container. we pick the container that want to deploy. there are some examples but we pick the custom one,
        so we should configure it. we click on configure and sidebar opens.here we configure howw ECS later executes "docker run".  it means
        all this options can we configure when we execute "docker run" command -
        here we give -

            name of container
            image based on which it should run
            port that should exposed

            in env section -
                entry point or command that should be xecuted wjen this container starts.
                working directory -

                envirenment variavles

            network settings - will come back to this when we have more than one container deployed.By default, our container will be 
                reachable from the web.

            storage and loggings - we can define bind mounts and volumes.

        now we defined our container that should be launched form here.

    tasks -
        Now, in the next section, we can define our task for this container.And the task is basically, as it says here,the blueprint for 
        your application.Here, you can tell AWS how it should launch your container.So not how it should execute docker run,but how the 
        server on which it runs this should be configured, you could say.And therefore a task actually also can include more than one 
        container.options here -

            compatibility - fargate or EC2.

        leave it untouched.

    service - And a service, in the end, now controls how this task,so this configured application and the container
        that belongs to it, should be executed. options -

        load balancer - we will not use it now.

        So you could say every task is executed by a service.So you have one service per task.

    cluster - 
        You could say that's the overall network in which our services run.Here, we have just one service with one task with one 
        container,but if you had a multi-container app,you could group multiple containers in one cluster so that they all belong together 
        logically, and they all can talk to each other.Now here, we automatically get this cluster network created for us


    after setting all these click on create , then on "view Service". now created service will be openen. how to see our running application -

        1)click on tasks. then click on single task(taskid not task defination).
        2)there under network, you can find public IP. paste this in browser and you will see your app up and running.

    but the huge advantage here is that we didn't start any custom servers or machines.We did not install anything, and we're not responsible
    for keeping anything up to date. We just configured how AWS should execute our containers,and that is it.


141)Updating managed container

    update your image and push it to docker hub. ow how to tell ECS to download the updated service -

        1)got to clusters -> default -> task -> task defination -> create new revision

            create new revision creates new task, keep all seetings same because we just want to create same task again.ECS will pull image form docker hub when you create
            a new task and then launch and use service in that tasks.

        2)click on "create" -> actions -> update service -> skip to review -> update service.

        3)now go to service -> tasks -> click on task id -> public ip -> open it in browser.

        now you will see that ip has chnaged, But there are ways of actually still connecting a domain to this running ECS task in general,
        independent from the specific IP AWS assigned,more on that can be found in the resources attached to this lecture.



    alternate -
        do niot create new task but use "update service" and select "Force new deployment"


142)Preparing a multiContainer App
    delete the previous setup -
        delete the service in cluster
        then delete the cluster


    refer to code 2. here we have 2 containers via docker compose.

    Important things -

        a)we will not use docker compose for deployment.Docker compose is a great tool for running multiple containers in a Docker
        compose file,possibly also for multiple containers on your local machine.But it's not really a great tool for deployment 
        right now. on deployment certain things start to matter which were not there in local like how CPU should be allocated to
        particular service.It depends heavily on the hosting provider you're using,which kind of extra information might be 
        needed. as this info cnt be part of docker compose.But that's no problem,you will see that deploying these individual 
        containers will still be super easy.And that we can use this compose file as an inspiration. To understand which deployment
        settings are needed for aws.So therefore, we'll just use docker-compose config file ,you could say an inspiration of what 
        we need to do and manually deploy these individual containers,these services to AWS ECS.

        b)Unfortunately for AWS ECS we will not be able to use this automatic find the container IP by container name feature
        which works so nice locally. this is because on local system, all contianer runs on same machine and docker creates the 
        network , all container are put on this network on same machine.and docker is able to replace the placeholder with actualt 
        IP of contianer. but on ECS there are thiusands of machine. it is highly unlikely that both your container will run on same
        machine.

        however if we use same task for them then they will run on same machine, Still then AWS ECS will not create a Docker network
        for them instead it allows you to use localhost as an address inside of your container application code.



        And you create a Docker network and then all these containers will be put into the same network on your local machine.
        And then AWS ECS gives us access to the network on that machine and to the other containers,which are part of the task,
        through the local host address.


        So we can use localhost instead of Mongo DB here when deploying this with AWS ECS,because these two containers,which we 
        wanna connect the node app container and Mongo DB will be part of the same task,and therefore will be guaranteed to run
        on the same machine.And then AWS ECS gives us access to the network on that machine and to the other containers,which are
        part of the task,through the local host address.

        but we do not want to change code when we deply i local and prod. for that we used value of mongodb from env file.
        for dev we give this value to MONGODB_UR in backaend.env.



143)configuring nodejs backend container
